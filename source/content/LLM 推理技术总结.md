---
title: LLM æ¨ç†æŠ€æœ¯æ€»ç»“
tags:
  - transformer
  - attention
  - llm
  - AI
  - å­¦ä¹ ç¬”è®°
date: 2025-10-28
---

# Batching ç­–ç•¥

æå‡GPUä½¿ç”¨ç‡æœ€ç®€å•çš„åŠæ³•å°±æ˜¯æ‰¹å¤„ç†ï¼ˆBatchingï¼‰ï¼Œå³å°†å¤šä¸ªè¯·æ±‚æ‹¼æ¥æˆä¸€ä¸ª Qã€Kã€V çŸ©é˜µä¸€æ¬¡æ€§åœ°å¤„ç†ã€‚
ä¼ ç»Ÿçš„Batchingï¼ˆstatic batchingï¼‰ç”±äºLLMå¯¹äºä¸åŒçš„è¯·æ±‚ç”Ÿæˆçš„tokenæ•°é‡ä¸ä¸€æ ·ï¼Œå¯èƒ½ä¼šå‡ºç°åŒä¸€ä¸ªæ‰¹æ¬¡é‡Œä¸åŒè¯·æ±‚é—´ç›¸äº’ç­‰å¾…ç›´åˆ°å…¨éƒ¨å®Œæˆåæ‰ä¸€èµ·è¿”å›çš„é—®é¢˜ã€‚

1. _continuous batching_ 
	1. https://insujang.github.io/2024-01-07/llm-inference-continuous-batching-and-pagedattention/
	2. https://www.anyscale.com/blog/continuous-batching-llm-inference
2. chunked prefill 

# å¹¶è¡Œç­–ç•¥

## æ•°æ®å¹¶è¡Œ

data parallelism ä¸»è¦æ˜¯ç”¨äºè®­ç»ƒæ—¶ï¼Œæƒé‡åœ¨å¤šä¸ªè®¾å¤‡é—´æ‹·è´ï¼Œé€šè¿‡å¢å¤§batchæ¥é™ä½è®­ç»ƒæ—¶é—´ã€‚

## æµæ°´çº¿å¹¶è¡Œ

æœ€å¤§çš„é—®é¢˜æ˜¯ä¸‹ä¸€ä¸ªè®¾å¤‡ç­‰å¾…ä¸Šä¸€ä¸ªè®¡ç®—çš„ç»“æœ (activations, gradients) çš„æ—¶å€™ä¼šç©ºé—²ï¼Œè¢«ç§°ä¸ºâ€œæ°”æ³¡â€ã€‚Microbatcing ï¼ˆå›¾cï¼‰å¯ä»¥å‡å°ä½†ä¸èƒ½æ¶ˆé™¤æ°”æ³¡ã€‚

![[Pasted image 20250628120154.png]]

## å¼ é‡å¹¶è¡Œ

Tensor parallelism involves sharding (horizontally) individual layers of the model into smaller, independent blocks of computation that can be executed on different devices. Attention blocks and multi-layer perceptron (MLP) layers are major components of transformers that can take advantage of tensor parallelism.

![[Pasted image 20250628120554.png]]

å¯ä»¥çœ‹åˆ° MLP å’Œ self-attention å¤©ç”Ÿå°±å¾ˆé€‚åˆå¹¶è¡Œï¼Œä½†æ˜¯åƒ LayerNorm å’Œ Dropout å‡½æ•°å¯¼è‡´éœ€è¦åœ¨ä¸åŒè®¾å¤‡é—´**å¤åˆ¶**ï¼ˆèšåˆä¸åŒblockè®¡ç®—çš„ç»“æœï¼‰ï¼Œè¿™å¯¼è‡´å†…å­˜è¦æ±‚æ›´é«˜ã€‚
## åºåˆ—å¹¶è¡Œ

å°†ä¸€æ®µè¾“å…¥åºåˆ—æ‹†åˆ†æˆä¸åŒæ®µï¼Œè®©æ¯ä¸ªGPUå¤„ç†åºåˆ—çš„ä¸€éƒ¨åˆ†ï¼Œç„¶åé€šè¿‡è·¨ GPU çš„é€šä¿¡ï¼ˆæ¯”å¦‚ all-to-allï¼‰å®ç° attention å’Œ residual è¿æ¥ç­‰æ“ä½œã€‚ï¼ˆå¯ä»¥çœ‹ä½œbatchingçš„åå‘æ“ä½œï¼Ÿï¼‰


# åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„ä¼˜åŒ–

## multi-query attention (MQA)

æ‰€æœ‰å¤´å…±ç”¨ä¸€ç»„ K/Vï¼Œ**ç¼“å­˜å‡å°‘çº¦ 8x~16x** ï¼Œä½†å¯èƒ½ä¼š **å‰Šå¼±æ³¨æ„åŠ›çš„è¡¨è¾¾èƒ½åŠ›**ã€‚ç„¶è€Œå®éªŒè¡¨æ˜ï¼Œåœ¨å¤§å¤šæ•°å®é™…ä»»åŠ¡ä¸­ï¼Œ**æŸå¤±çš„è¡¨è¾¾èƒ½åŠ›å¹¶ä¸ä¼šæ˜¾è‘—å½±å“æ•ˆæœ**ï¼›

https://arxiv.org/abs/1911.02150

## [Grouped-query attention](https://arxiv.org/pdf/2305.13245v2.pdf)Â (GQA)

MQA å’Œ MHA çš„æŠ˜ä¸­ç‰ˆï¼ŒÂ by projecting key and values to a few groups of query headsã€‚Models originally trained with MHA, can be â€œuptrainedâ€ with GQAã€‚

![[Pasted image 20250629092617.png]]

## Multi head Latent Attention

![[Pasted image 20250717103007.png]]

# åŸºäº KV Cache ç®¡ç†çš„ä¼˜åŒ–

## Flash Attention


![[Pasted image 20250716125252.png]]
from: https://huggingface.co/docs/text-generation-inference/conceptual/flash_attention
## Paged Attention

å› ä¸ºè¾“å…¥åºåˆ—çš„ä¸å¯é¢„æµ‹ï¼Œå†…å­˜æ€»æ˜¯éœ€è¦ä¿ç•™æ¨¡å‹æ”¯æŒçš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚åŸºäº Paging æœºåˆ¶å…è®¸å°†è¿ç»­çš„åºåˆ—å­˜å‚¨åˆ°ä¸è¿ç»­çš„ç©ºé—´ä¸­ï¼Œè€Œä¸”æ”¯æŒæŒ‰éœ€ç”³è¯·blockã€‚

## RadixAttention

ä½¿ç”¨æ ‘ï¼ˆradixï¼‰æ¥ç»´æŠ¤ kv cacheï¼Œä½¿ç”¨LRUçš„ç­–ç•¥æ¥å†³å®šç§»é™¤å“ªäº›cacheã€‚

![[Pasted image 20250716133210.png]]
Figure 4. Examples of RadixAttention operations with an LRU eviction policy, illustrated across nine steps.

å¯ä»¥ç»“åˆ Paged Attention å’Œ Continous Batching æ¥ä½¿ç”¨ã€‚
# æ¨¡å‹ä¼˜åŒ–æŠ€æœ¯

## é‡åŒ– Quantization

é‡åŒ–åˆåˆ†ä¸º reduced precision on either the activations, the weights, or bothã€‚

> activations ï¼ˆæ¿€æ´»å€¼ï¼‰æ˜¯æ¯ä¸€å±‚è®¡ç®—åçš„è¾“å‡ºå€¼ï¼Œè¾“å…¥ x â†’ ä¹˜ä»¥æƒé‡ W â†’ åŠ åç½® b â†’ æ¿€æ´»å‡½æ•° â†’ è¾“å‡º a
> **Weights æ˜¯æ¨¡å‹çš„â€œè®°å¿†â€ï¼ŒActivations æ˜¯æ¨¡å‹çš„â€œæ€è€ƒè¿‡ç¨‹â€**

å¯¹ weights é‡åŒ–æ˜¯å¾ˆæ˜¾è€Œæ˜“è§çš„ï¼Œé‡ç‚¹æ˜¯å¯¹ activations å¦‚ä½•å¤„ç†ã€‚ä¸€ç§æ–¹æ¡ˆæ˜¯åœ¨å°† weights å’Œ activations æ“ä½œæ—¶é‡æ–°è½¬æ¢æˆé«˜ç²¾åº¦çš„ã€‚ï¼ˆå› ä¸ºæ²¡æœ‰å¯¹ Â INT8 å’Œ FP16 ç›¸ä¹˜ä¼˜åŒ–çš„ç¡¬ä»¶ï¼‰

å¦ä¸€ç§æ–¹æ¡ˆæ˜¯å¯¹ activations ä¹Ÿé‡åŒ–ï¼Œä½†æ˜¯ç”±äºå®ƒç»å¸¸ä¼šåŒ…å«è¶…è¿‡è¾¹ç•Œçš„å€¼ï¼ˆoutliersï¼‰ï¼Œ

ç­–ç•¥ 1ï¼ˆLLM.int8() çš„æ–¹æ³•ï¼‰ï¼š

1. **ç”¨ä¸€ç»„å…¸å‹è¾“å…¥æ•°æ®**è·‘ä¸€éæ¨¡å‹ï¼ˆç§°ä¸º calibrationï¼‰ï¼›
2. æ‰¾å‡ºå“ªäº›å±‚æˆ–å“ªäº› token çš„ **activations ç»å¸¸å‡ºç°ç¦»ç¾¤å€¼**ï¼›    
3. å¯¹è¿™äº›éƒ¨åˆ†ç”¨æ›´é«˜ç²¾åº¦ï¼ˆå¦‚ FP16ï¼‰ä¿å­˜ï¼Œè€Œå…¶å®ƒéƒ¨åˆ†ä»ç”¨ INT8;
4. è¿™ç§æ–¹æ³•å°±æ˜¯è‘—åçš„ **LLM.int8() è®ºæ–‡æå‡ºçš„ mixed-precision é‡åŒ–**ã€‚

ç¬¬äºŒç§æ–¹æ³•æ˜¯ï¼š    
1. **weightsï¼ˆæƒé‡ï¼‰é€šå¸¸æ›´ç¨³å®šã€åˆ†å¸ƒæ›´å¯æ§**ï¼Œæ‰€ä»¥å¯ä»¥å…ˆå¯¹æƒé‡è¿›è¡Œé‡åŒ–ï¼Œå¾—åˆ°å…¶ **æœ€å°/æœ€å¤§å€¼ï¼ˆåŠ¨æ€èŒƒå›´ï¼‰**ã€‚ ç„¶å **ç”¨åŒæ ·çš„èŒƒå›´** æ¥å¯¹ activations è¿›è¡Œé‡åŒ–ï¼›
2. è¿™ä¸ªæ–¹æ¡ˆæœ¬è´¨æ˜¯ **æ”¾å¼ƒæ•æ‰æ¿€æ´»ä¸­çš„ç¦»ç¾¤æå€¼ï¼Œè½¬è€Œä¸“æ³¨ä¿ç•™ä¸»å¹²ä¿¡æ¯ç²¾åº¦**çš„ç­–ç•¥ï¼Œé€šè¿‡ä½¿ç”¨ç¨³å®šçš„å‚è€ƒå°ºåº¦ï¼Œé¿å… outlier æ‹‰å®½é‡åŒ–åŒºé—´ã€é€ æˆå…¨é¢ç²¾åº¦ä¸‹é™ã€‚

### ğŸ§  æ€»ç»“ä¸€ä¸‹ä¸¤ç§ç­–ç•¥ï¼š

|ç­–ç•¥|æè¿°|ä¼˜ç‚¹|æ–¹æ³•ä»£è¡¨|
|---|---|---|---|
|1. åŠ¨æ€æŒ‘é€‰|æ‰¾å‡ºå“ªäº› activations éœ€è¦é«˜ç²¾åº¦è¡¨ç¤ºï¼Œåªå¯¹é‚£éƒ¨åˆ†æé«˜ç²¾åº¦|ç²¾åº¦é«˜ï¼Œé€‚åº”æ€§å¼º|LLM.int8()|
|2. å€Ÿæƒé‡èŒƒå›´|ç”¨ weight çš„é‡åŒ–èŒƒå›´å¥—ç”¨åœ¨ activation ä¸Š|ç®€å•ï¼Œä¸éœ€è¦é¢å¤–æ•°æ®è·‘æ¨¡å‹|æœ‰äº›æ¨ç†å¼•æ“çš„é™æ€é‡åŒ–|


![[Pasted image 20250629150220.png]]

## ç¨€ç–çŸ©é˜µ 

GPUs in particular have hardware acceleration for a certain kind ofÂ _structured sparsity_, where two out of every four values are represented by zeros.

## è’¸é¦

![[Pasted image 20250629152011.png]]


# æ¨ç†ä¼˜åŒ–

## Continous batching

This takes advantage of the fact that the overall text generation process for an LLM can be broken down into multiple iterations of execution on the model. æ¨ç†æœåŠ¡å™¨å¯ä»¥æå‰å°†å·²ç»å®Œæˆçš„batchä»è®¡ç®—ä¸­é©±é€è€Œä¿æŒæœªå®Œæˆçš„ç»§ç»­è®¡ç®—ã€‚ç®—æ˜¯å·¥ç¨‹ä¸Šçš„ä¼˜åŒ–ã€‚

## Speculative inference

Â a draft model temporarily predicts multiple future steps that are verified or rejected in parallel

![[Pasted image 20250629153344.png]]

draft modelæ˜¯ä¸²è¡Œçš„ï¼Œä½†æ˜¯ verify modelå¯ä»¥å¹¶è¡Œï¼Œé€šå¸¸ç”¨ä¸€ä¸ªä¾¿å®œçš„ä½œä¸ºdraftï¼Œç”¨å¤§çš„ä½œä¸ºverify

## Auto Prefix Caching

KV Cache ä¸»è¦æ˜¯ç”¨decodeé˜¶æ®µåŠ é€Ÿï¼Œé¿å…äº†æ¯æ¬¡ç”Ÿæˆtokenéƒ½è¦é‡æ–°è®¡ç®—Kã€Qã€Vã€‚ vLLMçš„ [automatic prefix caching](https://docs.vllm.ai/en/latest/features/automatic_prefix_caching.html) å¯ä»¥è·³è¿‡éƒ¨åˆ† Prefill çš„é˜¶æ®µã€‚å½“è¯·æ±‚çš„æ–‡æœ¬å‰ç¼€ç›¸åŒæ—¶ï¼Œä¼šç›´æ¥å¤ç”¨ä¹‹å‰çš„ KV Cacheï¼Œä¸ç”¨ Prefill é˜¶æ®µå†é‡æ–°è®¡ç®—äº†ã€‚è¿™ç§è·¨è¯·æ±‚çº§åˆ«çš„ç¼“å­˜æœºåˆ¶ä½¿å¾—æœåŠ¡åœ¨åº”å¯¹é«˜å¹¶å‘çš„åœºæ™¯ä¸‹æé«˜äº†ååé‡ã€‚æƒ³åƒä¸¤ç§åœºæ™¯ï¼š
1. ç”¨æˆ·é‡å¤å¯¹ä¸€æ®µé•¿æ–‡æœ¬è¿›è¡Œæé—®ï¼ˆRAGï¼‰ï¼›
2. å¤šè½®å¯¹è¯ï¼›

è¿™ä¸¤ç§åœºæ™¯ä¸‹æ¯æ¬¡è¯·æ±‚çš„å‰ç¼€æ˜¯ç›¸åŒçš„ï¼Œå› æ­¤å¯ä»¥ç›´æ¥å¤ç”¨ä¸Šä¸€è½®çš„ç¼“å­˜ã€‚


# å‚è€ƒ
- https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/
- 